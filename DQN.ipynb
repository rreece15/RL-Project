{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box, Discrete\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "    def __init__(self,game_state='Champion.Level1.RyuVsGuile.state',record=False):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0,high=255,shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(2**12)\n",
    "        self.total_matches_won = 0\n",
    "        self.total_enemy_matches_won = 0\n",
    "        self.prev_matches_won = 0\n",
    "        self.prev_enemy_matches_won = 0\n",
    "        self.record = record\n",
    "        # if record:\n",
    "        #     self.game = retro.make(game='StreetFighterII-Champion', record='.',state=game_state,use_restricted_actions=retro.Actions.DISCRETE)\n",
    "        # else:\n",
    "        self.game = retro.make(game='StreetFighterII-Champion', state=game_state,use_restricted_actions=retro.Actions.DISCRETE)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs_orig = obs\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        if self.prev_matches_won > info['matches_won']:\n",
    "            self.prev_matches_won = info['matches_won']\n",
    "        if self.prev_enemy_matches_won > info['enemy_matches_won']:\n",
    "            self.prev_enemy_matches_won = info['enemy_matches_won']\n",
    "        self.total_matches_won = self.total_matches_won + info['matches_won'] - self.prev_matches_won\n",
    "        self.total_enemy_matches_won = self.total_enemy_matches_won + info['enemy_matches_won'] - self.prev_enemy_matches_won\n",
    "        self.prev_matches_won = info['matches_won']\n",
    "        self.prev_enemy_matches_won = info['enemy_matches_won']\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        self.obs_orig = obs_orig\n",
    "        return frame_delta, reward, done, info\n",
    "        \n",
    "    def get_obs_orig(self):\n",
    "        return self.obs_orig\n",
    "\n",
    "    def render(self,*args,**kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        self.score = 0\n",
    "        self.prev_matches_won = 0\n",
    "        self.prev_enemy_matches_won = 0\n",
    "        return obs\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter(game_state='Hyper.Level1.RyuVsDhalsim.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_qrdqn\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "action = env.action_space.sample()\n",
    "while True:\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # if reward != 0:\n",
    "    #     print(reward)\n",
    "    #     print(info)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "    action = env.action_space.sample()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "from stable_baselines3 import DQN\n",
    "# from sb3_contrib import QRDQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/DQN/RyuHyper'\n",
    "OPT_DIR = './opt/DQN/RyuHyper'\n",
    "CHECKPOINT_DIR = './train/DQN/RyuHyper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(trial):\n",
    "    return {\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate',1e-5,1e-4),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8,0.9999),\n",
    "        'tau':trial.suggest_loguniform('tau', 0.001,0.01),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize(trial)\n",
    "        env = StreetFighter(game_state='Champion.Level1.MBisonVsKen.state')\n",
    "        env = Monitor(env,LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env,4,channels_order='last')\n",
    "\n",
    "        model = DQN(\"CnnPolicy\",env,tensorboard_log=LOG_DIR,verbose=0,batch_size=256,buffer_size=80000, **model_params) # cnn policy uses conv neural net for \n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        mean_reward, _ = evaluate_policy(model,env,n_eval_episodes=10)\n",
    "        env.close()\n",
    "        \n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        print(mean_reward)\n",
    "        \n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 01:39:12,652] A new study created in memory with name: no-name-c3a7d7a0-d597-4169-b825-1a68c3028cec\n",
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"\"\"\n",
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:276: UserWarning: Path 'opt\\DQN\\Balrog' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n",
      "[I 2024-04-26 01:45:54,859] Trial 0 finished with value: 30000.0 and parameters: {'learning_rate': 4.257797176934804e-05, 'gamma': 0.9930042618672252, 'tau': 0.0029921081489775623}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 01:50:55,104] Trial 1 finished with value: 2800.0 and parameters: {'learning_rate': 1.7887245961766925e-05, 'gamma': 0.9036578312345488, 'tau': 0.008457484056831197}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 01:54:32,854] Trial 2 finished with value: 600.0 and parameters: {'learning_rate': 8.323482655912327e-05, 'gamma': 0.8965641256809231, 'tau': 0.0014913904861179003}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 01:57:08,222] Trial 3 finished with value: 0.0 and parameters: {'learning_rate': 6.418054239844212e-05, 'gamma': 0.8913201680257214, 'tau': 0.003702351016578276}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 02:07:17,341] Trial 4 finished with value: 19800.0 and parameters: {'learning_rate': 1.9302743639428782e-05, 'gamma': 0.8842604834816046, 'tau': 0.002585490537379548}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19800.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 02:14:38,220] Trial 5 finished with value: 13700.0 and parameters: {'learning_rate': 2.9667755008135658e-05, 'gamma': 0.8236520518137667, 'tau': 0.0014328933400145102}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13700.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 02:19:55,540] Trial 6 finished with value: 1300.0 and parameters: {'learning_rate': 1.6765398902473234e-05, 'gamma': 0.9207857707481593, 'tau': 0.0014409219949906415}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 02:24:02,940] Trial 7 finished with value: 4300.0 and parameters: {'learning_rate': 1.4943354561910721e-05, 'gamma': 0.8774758020389098, 'tau': 0.001734610979691799}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 02:32:10,411] Trial 8 finished with value: 28400.0 and parameters: {'learning_rate': 2.2839167556398748e-05, 'gamma': 0.8263634392651135, 'tau': 0.005126092224281044}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28400.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-26 02:39:57,325] Trial 9 finished with value: 13200.0 and parameters: {'learning_rate': 5.409781672519991e-05, 'gamma': 0.9070773075744816, 'tau': 0.0014010944800243474}. Best is trial 0 with value: 30000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13200.0\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent,n_trials=10,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[30000.0], datetime_start=datetime.datetime(2024, 4, 26, 1, 39, 12, 654753), datetime_complete=datetime.datetime(2024, 4, 26, 1, 45, 54, 859033), params={'learning_rate': 4.257797176934804e-05, 'gamma': 0.9930042618672252, 'tau': 0.0029921081489775623}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.0001, log=True, low=1e-05, step=None), 'gamma': FloatDistribution(high=0.9999, log=True, low=0.8, step=None), 'tau': FloatDistribution(high=0.01, log=True, low=0.001, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 4.257797176934804e-05,\n",
       " 'gamma': 0.9930042618672252,\n",
       " 'tau': 0.0029921081489775623}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback,self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}_'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            self.logger.record('matches_won', self.training_env.get_attr('total_matches_won')[0])\n",
    "            self.logger.record('enemy_matches_won', self.training_env.get_attr('total_enemy_matches_won')[0])\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000,save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter(game_state='Hyper.Level1.RyuVsDhalsim.state')\n",
    "env = Monitor(env,LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env,4,channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "model_params['learning_rate'] = 4.257797176934804e-05\n",
    "model_params['gamma'] = 0.9930042618672252\n",
    "model_params['tau'] = 0.0029921081489775623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/DQN/RyuHyper\\DQN_12\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 10       |\n",
      "| matches_won         | 16       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52e+04 |\n",
      "|    ep_rew_mean      | 5.54e+04 |\n",
      "|    exploration_rate | 0.423    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 865      |\n",
      "|    total_timesteps  | 60703    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.385    |\n",
      "|    n_updates        | 2675     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 19       |\n",
      "| matches_won         | 21       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.24e+04 |\n",
      "|    ep_rew_mean      | 4.33e+04 |\n",
      "|    exploration_rate | 0.0551   |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 25       |\n",
      "|    time_elapsed     | 3890     |\n",
      "|    total_timesteps  | 99467    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 12.1     |\n",
      "|    n_updates        | 12366    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 27       |\n",
      "| matches_won         | 28       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.13e+04 |\n",
      "|    ep_rew_mean      | 3.52e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 19       |\n",
      "|    time_elapsed     | 6839     |\n",
      "|    total_timesteps  | 135848   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 3.9      |\n",
      "|    n_updates        | 21461    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 35       |\n",
      "| matches_won         | 31       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.06e+04 |\n",
      "|    ep_rew_mean      | 3.13e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 17       |\n",
      "|    time_elapsed     | 9756     |\n",
      "|    total_timesteps  | 169717   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 2.18     |\n",
      "|    n_updates        | 29929    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 44       |\n",
      "| matches_won         | 34       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.99e+03 |\n",
      "|    ep_rew_mean      | 2.89e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 16       |\n",
      "|    time_elapsed     | 12265    |\n",
      "|    total_timesteps  | 199891   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 6.15e-05 |\n",
      "|    n_updates        | 37472    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 54       |\n",
      "| matches_won         | 39       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.71e+03 |\n",
      "|    ep_rew_mean      | 2.54e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 15       |\n",
      "|    time_elapsed     | 14871    |\n",
      "|    total_timesteps  | 232927   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 7.69e-05 |\n",
      "|    n_updates        | 45731    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 60       |\n",
      "| matches_won         | 39       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.19e+03 |\n",
      "|    ep_rew_mean      | 2.2e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 15       |\n",
      "|    time_elapsed     | 16791    |\n",
      "|    total_timesteps  | 257325   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 1.95     |\n",
      "|    n_updates        | 51831    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 70       |\n",
      "| matches_won         | 39       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.77e+03 |\n",
      "|    ep_rew_mean      | 1.97e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 18742    |\n",
      "|    total_timesteps  | 280569   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 3.21e-05 |\n",
      "|    n_updates        | 57642    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 76       |\n",
      "| matches_won         | 40       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.55e+03 |\n",
      "|    ep_rew_mean      | 1.81e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 20905    |\n",
      "|    total_timesteps  | 307680   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 5.07e-05 |\n",
      "|    n_updates        | 64419    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 84       |\n",
      "| matches_won         | 42       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.44e+03 |\n",
      "|    ep_rew_mean      | 1.71e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 23613    |\n",
      "|    total_timesteps  | 337577   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.389    |\n",
      "|    n_updates        | 71894    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 93       |\n",
      "| matches_won         | 43       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.27e+03 |\n",
      "|    ep_rew_mean      | 1.59e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 25919    |\n",
      "|    total_timesteps  | 363895   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.000278 |\n",
      "|    n_updates        | 78473    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 100      |\n",
      "| matches_won         | 43       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.09e+03 |\n",
      "|    ep_rew_mean      | 1.48e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 27962    |\n",
      "|    total_timesteps  | 388410   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 4.1e-05  |\n",
      "|    n_updates        | 84602    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 110      |\n",
      "| matches_won         | 43       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.92e+03 |\n",
      "|    ep_rew_mean      | 1.38e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 29987    |\n",
      "|    total_timesteps  | 412022   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 3.37e-05 |\n",
      "|    n_updates        | 90505    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 116      |\n",
      "| matches_won         | 43       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.79e+03 |\n",
      "|    ep_rew_mean      | 1.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 32030    |\n",
      "|    total_timesteps  | 436078   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 3.37e-05 |\n",
      "|    n_updates        | 96519    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 125      |\n",
      "| matches_won         | 45       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.83e+03 |\n",
      "|    ep_rew_mean      | 1.26e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 34974    |\n",
      "|    total_timesteps  | 469658   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.0068   |\n",
      "|    n_updates        | 104914   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 133      |\n",
      "| matches_won         | 48       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.81e+03 |\n",
      "|    ep_rew_mean      | 1.23e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 37637    |\n",
      "|    total_timesteps  | 499741   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 3.85e-05 |\n",
      "|    n_updates        | 112435   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 143      |\n",
      "| matches_won         | 48       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.7e+03  |\n",
      "|    ep_rew_mean      | 1.17e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 39742    |\n",
      "|    total_timesteps  | 523582   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 4.74e-05 |\n",
      "|    n_updates        | 118395   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 150      |\n",
      "| matches_won         | 51       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.75e+03 |\n",
      "|    ep_rew_mean      | 1.14e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 43178    |\n",
      "|    total_timesteps  | 557772   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 2.7      |\n",
      "|    n_updates        | 126942   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 162      |\n",
      "| matches_won         | 54       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.79e+03 |\n",
      "|    ep_rew_mean      | 1.13e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 46650    |\n",
      "|    total_timesteps  | 592409   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.389    |\n",
      "|    n_updates        | 135602   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 170      |\n",
      "| matches_won         | 56       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.79e+03 |\n",
      "|    ep_rew_mean      | 1.11e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 49824    |\n",
      "|    total_timesteps  | 623390   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.346    |\n",
      "|    n_updates        | 143347   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 178      |\n",
      "| matches_won         | 57       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.75e+03 |\n",
      "|    ep_rew_mean      | 1.08e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 52662    |\n",
      "|    total_timesteps  | 650962   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.389    |\n",
      "|    n_updates        | 150240   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 184      |\n",
      "| matches_won         | 58       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.71e+03 |\n",
      "|    ep_rew_mean      | 1.05e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 55457    |\n",
      "|    total_timesteps  | 678241   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 6.57e-05 |\n",
      "|    n_updates        | 157060   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 192      |\n",
      "| matches_won         | 60       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.7e+03  |\n",
      "|    ep_rew_mean      | 1.05e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 58490    |\n",
      "|    total_timesteps  | 708812   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 6.42e-05 |\n",
      "|    n_updates        | 164702   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 203      |\n",
      "| matches_won         | 60       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.64e+03 |\n",
      "|    ep_rew_mean      | 1.01e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 60624    |\n",
      "|    total_timesteps  | 733292   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 16.5     |\n",
      "|    n_updates        | 170822   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 211      |\n",
      "| matches_won         | 61       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.61e+03 |\n",
      "|    ep_rew_mean      | 9.89e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 62967    |\n",
      "|    total_timesteps  | 760588   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 8.39e-05 |\n",
      "|    n_updates        | 177646   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 219      |\n",
      "| matches_won         | 63       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.31e+03 |\n",
      "|    ep_rew_mean      | 7.9e+03  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 65782    |\n",
      "|    total_timesteps  | 791457   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 8.98e-05 |\n",
      "|    n_updates        | 185364   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 226      |\n",
      "| matches_won         | 63       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.16e+03 |\n",
      "|    ep_rew_mean      | 6.73e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 67647    |\n",
      "|    total_timesteps  | 815149   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 8.76e-05 |\n",
      "|    n_updates        | 191287   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 233      |\n",
      "| matches_won         | 63       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.03e+03 |\n",
      "|    ep_rew_mean      | 6.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 12       |\n",
      "|    time_elapsed     | 69827    |\n",
      "|    total_timesteps  | 839310   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 9.2e-05  |\n",
      "|    n_updates        | 197327   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 242      |\n",
      "| matches_won         | 63       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.98e+03 |\n",
      "|    ep_rew_mean      | 5.51e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 72879    |\n",
      "|    total_timesteps  | 868059   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.000113 |\n",
      "|    n_updates        | 204514   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 251      |\n",
      "| matches_won         | 65       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.96e+03 |\n",
      "|    ep_rew_mean      | 4.84e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 75758    |\n",
      "|    total_timesteps  | 895651   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.000117 |\n",
      "|    n_updates        | 211412   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 259      |\n",
      "| matches_won         | 67       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.9e+03  |\n",
      "|    ep_rew_mean      | 4.76e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 78833    |\n",
      "|    total_timesteps  | 922977   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 218244   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 267      |\n",
      "| matches_won         | 68       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.93e+03 |\n",
      "|    ep_rew_mean      | 4.89e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 81186    |\n",
      "|    total_timesteps  | 950619   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 225154   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| enemy_matches_won   | 273      |\n",
      "| matches_won         | 70       |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.96e+03 |\n",
      "|    ep_rew_mean      | 5e+03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 11       |\n",
      "|    time_elapsed     | 83630    |\n",
      "|    total_timesteps  | 976726   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 4.26e-05 |\n",
      "|    loss             | 0.000119 |\n",
      "|    n_updates        | 231681   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1ef9428c388>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\"CnnPolicy\",env,tensorboard_log=LOG_DIR,verbose=2,batch_size=256,buffer_size=80000, **model_params) # cnn policy uses conv neural net for \n",
    "# model.load(os.path.join(OPT_DIR, 'trial_0_best_model.zip'))\n",
    "model.load(os.path.join(OPT_DIR, 'trial_2_best_model.zip'))\n",
    "model.learn(total_timesteps=1000000,callback=callback)\n",
    "# can increase training time and learning rate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter(game_state='Champion.Level1.RyuVsGuile.state')\n",
    "env = Monitor(env,LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env,4,channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(os.path.join(CHECKPOINT_DIR, 'best_model_1000000_'))\n",
    "# model = QRDQN.load(os.path.join('./train/QRDQN/', 'best_model_1000000_'))\n",
    "# Ken best (steps - reward) 1000000 - 1400,  50000 - 3300\n",
    "# Guile best (steps - reward) 1000000 - 2700, \n",
    "# Zangief best (steps - reward) 1000000 - 18400, 58000 - 10300\n",
    "# Chun-Li best (steps - reward) 1000000 - 23200, 500000 - 16900, 50000 = 11400\n",
    "# Balrog best (steps - reward) 50000 - 2500, 600000 - 800, 1,000,000 - 0\n",
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)\n",
    "print(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:220: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 4.52GB > 3.73GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n",
      "c:\\Users\\reece\\anaconda3\\envs\\streetfighter37_copy3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAACoCAYAAAB64tvbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAACBElEQVR4nO3ZsQ2EQAwAwb8X/bdsKoCEFQg0kzpxsnLgNTPzAy77P70AfIWYICImiIgJImKCiJggsp0N11p37QGvcfRNcpkgIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYILKdDWfmrj3g9VwmiIgJImKCiJggIiaIiAkiOy0pC03Pyv2PAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 256x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensure you have imagemagick installed with \n",
    "sudo apt-get install imagemagick\n",
    "Open file in CLI with:\n",
    "xgd-open <filelname>\n",
    "\"\"\"\n",
    "\n",
    "#Make gym env\n",
    "my_env = StreetFighter(game_state='Hyper.Level1.RyuVsDhalsim.state')\n",
    "env = Monitor(my_env,LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env,4,channels_order='last')\n",
    "\n",
    "model = DQN.load(os.path.join(CHECKPOINT_DIR, 'best_model_1000000_'))\n",
    "obs = env.reset()\n",
    "done = False\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    if done:\n",
    "        break\n",
    "    # frames.append(env.render(mode=\"rgb_array\"))\n",
    "    env.render()\n",
    "    obs,reward,done,info = env.step(model.predict(obs)[0])\n",
    "    rec_obs = my_env.get_obs_orig()\n",
    "    frames.append(rec_obs)\n",
    "env.close()\n",
    "\n",
    "save_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "street_fighter37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
