{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box, Discrete\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "    def __init__(self,game_state='Champion.Level1.RyuVsGuile.state'):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0,high=255,shape=(84,84,1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(2**12)\n",
    "        self.game = retro.make(game='StreetFighterII-Champion', state=game_state,use_restricted_actions=retro.Actions.DISCRETE)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        return frame_delta, reward, done, info\n",
    "\n",
    "    def render(self,*args,**kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        self.score = 0\n",
    "        return obs\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84,84), cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84,84,1))\n",
    "        return channels\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "action = env.action_space.sample()\n",
    "while True:\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # if reward != 0:\n",
    "    #     print(reward)\n",
    "    #     print(info)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "    action = env.action_space.sample()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(trial):\n",
    "    return {\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate',1e-5,1e-4),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8,0.9999),\n",
    "        'tau':trial.suggest_loguniform('tau', 0.001,0.01),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize(trial)\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env,LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env,4,channels_order='last')\n",
    "\n",
    "        model = DQN(\"CnnPolicy\",env,tensorboard_log=LOG_DIR,verbose=0,batch_size=216,buffer_size=80000, **model_params) # cnn policy uses conv neural net for \n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        mean_reward, _ = evaluate_policy(model,env,n_eval_episodes=10)\n",
    "        env.close()\n",
    "        \n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        print(mean_reward)\n",
    "        \n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 01:43:36,766] A new study created in memory with name: no-name-e9ad6619-610f-4640-896c-42625f94869e\n",
      "c:\\Users\\reece\\anaconda3\\envs\\street_fighter37\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\Users\\reece\\anaconda3\\envs\\street_fighter37\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\Users\\reece\\anaconda3\\envs\\street_fighter37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"\"\"\n",
      "[I 2024-04-17 01:53:32,165] Trial 0 finished with value: 43000.0 and parameters: {'learning_rate': 1.0441637166392424e-05, 'gamma': 0.8747694471766431, 'tau': 0.004366731241391021}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reece\\anaconda3\\envs\\street_fighter37\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\Users\\reece\\anaconda3\\envs\\street_fighter37\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\Users\\reece\\anaconda3\\envs\\street_fighter37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"\"\"\n",
      "[I 2024-04-17 01:57:55,204] Trial 1 finished with value: 3600.0 and parameters: {'learning_rate': 7.968728754174326e-05, 'gamma': 0.8133541902290516, 'tau': 0.002005755104267706}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:02:12,382] Trial 2 finished with value: 2700.0 and parameters: {'learning_rate': 6.744269577862206e-05, 'gamma': 0.817254794707618, 'tau': 0.0030184665847828303}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:05:55,095] Trial 3 finished with value: 2500.0 and parameters: {'learning_rate': 1.1245882391457875e-05, 'gamma': 0.8870886071389116, 'tau': 0.001591208035679002}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:15:18,534] Trial 4 finished with value: 8800.0 and parameters: {'learning_rate': 1.3297085592279778e-05, 'gamma': 0.8158765500452622, 'tau': 0.0017349746032888499}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8800.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:18:31,436] Trial 5 finished with value: 2500.0 and parameters: {'learning_rate': 2.348534473024836e-05, 'gamma': 0.9228256618551754, 'tau': 0.006977873816395121}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:24:42,872] Trial 6 finished with value: 15500.0 and parameters: {'learning_rate': 1.0803167515577025e-05, 'gamma': 0.9749308964328317, 'tau': 0.0018762102897521594}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:28:57,689] Trial 7 finished with value: 4900.0 and parameters: {'learning_rate': 4.4051584256391133e-05, 'gamma': 0.9888577470821505, 'tau': 0.001892289276111243}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:34:53,490] Trial 8 finished with value: 23200.0 and parameters: {'learning_rate': 3.7421915682778476e-05, 'gamma': 0.8550033281050156, 'tau': 0.003183241726434877}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-17 02:38:44,004] Trial 9 finished with value: 5200.0 and parameters: {'learning_rate': 5.628977335298013e-05, 'gamma': 0.9092951046959884, 'tau': 0.0030305924726291736}. Best is trial 0 with value: 43000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200.0\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent,n_trials=10,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[43000.0], datetime_start=datetime.datetime(2024, 4, 17, 1, 43, 36, 768013), datetime_complete=datetime.datetime(2024, 4, 17, 1, 53, 32, 165284), params={'learning_rate': 1.0441637166392424e-05, 'gamma': 0.8747694471766431, 'tau': 0.004366731241391021}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.0001, log=True, low=1e-05, step=None), 'gamma': FloatDistribution(high=0.9999, log=True, low=0.8, step=None), 'tau': FloatDistribution(high=0.01, log=True, low=0.001, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1.0441637166392424e-05,\n",
       " 'gamma': 0.8747694471766431,\n",
       " 'tau': 0.004366731241391021}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning(\n",
    "        env, # openai gym environment\n",
    "        alpha:float, # step size\n",
    "        gamma:float,\n",
    "        num_episode:int\n",
    ") -> np.array:\n",
    "    def epsilon_greedy_policy(s,done,w,epsilon=.0):\n",
    "        nA = env.action_space.n\n",
    "        Q = [np.dot(w, X(s,done,a)) for a in range(nA)]\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(nA)\n",
    "        else:\n",
    "            return np.argmax(Q)\n",
    "        \n",
    "\n",
    "    # Loop for each episode\n",
    "    for ep in range(num_episode):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "        # Loop until terminal\n",
    "\n",
    "            # Choose action from S using policy derived from Q\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "\n",
    "            # update Q\n",
    "\n",
    "            # update S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "street_fighter37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
